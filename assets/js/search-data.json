{
  
    
        "post0": {
            "title": "DECISION TREE (Titanic dataset)",
            "content": "Table of Contents . Problem Statement | Data Loading and Description | Preprocessing | Decision Tree 4.1 Introduction of Decision Tree | 4.2 Important Terminology related to Decision Trees | 4.3 Types of Decision Trees | 4.4 Concept of Homogenity | 4.5 How does a tree decide where to split? 4.5.1 Gini Index | 4.5.2 Information Gain | . | 4.6 Advantages of using Decision Tree | 4.7 Shortcomings of Decision Trees | 4.8 Preparing X and y using pandas | 4.9 Splitting X and y into training and test datasets. | 4.10 Decision Tree in scikit-learn | 4.11 Using the Model for Prediction | . | Model evaluation 5.1 Model Evaluation using accuracy score | 5.2 Model Evaluation using confusion matrix | . | Decision Tree with Gridsearch | . 1. Problem Statement . The goal is to predict survival of passengers travelling in RMS Titanic using Logistic regression. . . 2. Data Loading and Description . The dataset consists of the information about people boarding the famous RMS Titanic. Various variables present in the dataset includes data of age, sex, fare, ticket etc. | The dataset comprises of 891 observations of 12 columns. Below is a table showing names of all the columns and their description. | . Column Name Description . PassengerId | Passenger Identity | . Survived | Whether passenger survived or not | . Pclass | Class of ticket | . Name | Name of passenger | . Sex | Sex of passenger | . Age | Age of passenger | . SibSp | Number of sibling and/or spouse travelling with passenger | . Parch | Number of parent and/or children travelling with passenger | . Ticket | Ticket number | . Fare | Price of ticket | . Cabin | Cabin number | . Embarked | Gate of embarmkment | . Importing packages . import sys !{sys.executable} -m pip install pandas-profiling . import numpy as np # Implemennts milti-dimensional array and matrices import pandas as pd # For data manipulation and analysis import pandas_profiling import matplotlib.pyplot as plt # Plotting library for Python programming language and it&#39;s numerical mathematics extension NumPy import seaborn as sns # Provides a high level interface for drawing attractive and informative statistical graphics %matplotlib inline sns.set() from subprocess import check_output . Importing the Dataset . titanic_data = pd.read_csv(&quot;DT/titanic_train.csv&quot;) # Importing training dataset using pd.read_csv . titanic_data.head(10) . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . 5 6 | 0 | 3 | Moran, Mr. James | male | NaN | 0 | 0 | 330877 | 8.4583 | NaN | Q | . 6 7 | 0 | 1 | McCarthy, Mr. Timothy J | male | 54.0 | 0 | 0 | 17463 | 51.8625 | E46 | S | . 7 8 | 0 | 3 | Palsson, Master. Gosta Leonard | male | 2.0 | 3 | 1 | 349909 | 21.0750 | NaN | S | . 8 9 | 1 | 3 | Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) | female | 27.0 | 0 | 2 | 347742 | 11.1333 | NaN | S | . 9 10 | 1 | 2 | Nasser, Mrs. Nicholas (Adele Achem) | female | 14.0 | 1 | 0 | 237736 | 30.0708 | NaN | C | . titanic_data.isnull().sum() . PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 . . 3. Preprocessing the data . Dealing with missing values Dropping/Replacing missing entries of Embarked. | Replacing missing values of Age with median values. | Dropping the column &#39;Cabin&#39; as it has too many null values. | Replacing 0 values of fare with median values. | . | . titanic_data.Embarked = titanic_data.Embarked.fillna(titanic_data[&#39;Embarked&#39;].mode()[0]) . median_age = titanic_data.Age.median() titanic_data.Age.fillna(median_age, inplace = True) . titanic_data.drop(&#39;Cabin&#39;, axis = 1,inplace = True) . titanic_data[&#39;Fare&#39;]=titanic_data[&#39;Fare&#39;].replace(0,titanic_data[&#39;Fare&#39;].median()) . Creating a new feature named FamilySize. | . titanic_data[&#39;FamilySize&#39;] = titanic_data[&#39;SibSp&#39;] + titanic_data[&#39;Parch&#39;]+1 . Segmenting Sex column as per Age, Age less than 15 as Child, Age greater than 15 as Males and Females as per their gender. | . titanic_data[&#39;GenderClass&#39;] = titanic_data.apply(lambda x: &#39;child&#39; if x[&#39;Age&#39;] &lt; 15 else x[&#39;Sex&#39;],axis=1) . titanic_data[titanic_data.Age&lt;15].head(2) . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Embarked FamilySize GenderClass . 7 8 | 0 | 3 | Palsson, Master. Gosta Leonard | male | 2.0 | 3 | 1 | 349909 | 21.0750 | S | 5 | child | . 9 10 | 1 | 2 | Nasser, Mrs. Nicholas (Adele Achem) | female | 14.0 | 1 | 0 | 237736 | 30.0708 | C | 2 | child | . titanic_data[titanic_data.Age&gt;15].head(2) . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Embarked FamilySize GenderClass . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | S | 2 | male | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C | 2 | female | . Dummification of GenderClass &amp; Embarked. | . titanic_data = pd.get_dummies(titanic_data, columns=[&#39;GenderClass&#39;,&#39;Embarked&#39;], drop_first=True) . Dropping columns &#39;Name&#39; , &#39;Ticket&#39; , &#39;Sex&#39; , &#39;SibSp&#39; and &#39;Parch&#39; | . titanic = titanic_data.drop([&#39;Name&#39;,&#39;Ticket&#39;,&#39;Sex&#39;,&#39;SibSp&#39;,&#39;Parch&#39;], axis = 1) titanic.head() . PassengerId Survived Pclass Age Fare FamilySize GenderClass_female GenderClass_male Embarked_Q Embarked_S . 0 1 | 0 | 3 | 22.0 | 7.2500 | 2 | 0 | 1 | 0 | 1 | . 1 2 | 1 | 1 | 38.0 | 71.2833 | 2 | 1 | 0 | 0 | 0 | . 2 3 | 1 | 3 | 26.0 | 7.9250 | 1 | 1 | 0 | 0 | 1 | . 3 4 | 1 | 1 | 35.0 | 53.1000 | 2 | 1 | 0 | 0 | 1 | . 4 5 | 0 | 3 | 35.0 | 8.0500 | 1 | 0 | 1 | 0 | 1 | . Drawing pair plot to know the joint relationship between &#39;Fare&#39; , &#39;Age&#39; , &#39;Pclass&#39; &amp; &#39;Survived&#39; . sns.pairplot(titanic_data[[&quot;Fare&quot;,&quot;Age&quot;,&quot;Pclass&quot;,&quot;Survived&quot;]],vars = [&quot;Fare&quot;,&quot;Age&quot;,&quot;Pclass&quot;],hue=&quot;Survived&quot;, dropna=True,markers=[&quot;o&quot;, &quot;s&quot;]) plt.title(&#39;Pair Plot&#39;) . Text(0.5, 1.0, &#39;Pair Plot&#39;) . Observing the diagonal elements, . More people of Pclass 1 survived than died (First peak of red is higher than blue) | More people of Pclass 3 died than survived (Third peak of blue is higher than red) | More people of age group 20-40 died than survived. | Most of the people paying less fare died. | . Establishing coorelation between all the features using heatmap. . corr = titanic_data.corr() plt.figure(figsize=(10,10)) sns.heatmap(corr,vmax=.8,linewidth=.01, square = True, annot = True,cmap=&#39;YlGnBu&#39;,linecolor =&#39;black&#39;) plt.title(&#39;Correlation between features&#39;) . Text(0.5, 1.0, &#39;Correlation between features&#39;) . Age and Pclass are negatively corelated with Survived. | FamilySize is made from Parch and SibSb only therefore high positive corelation among them. | Fare and FamilySize are positively coorelated with Survived. | With high corelation we face redundancy issues. | . . 4. Decision Tree . . 4.1 Introduction of Decision Tree . A decision tree is one of most frequently and widely used supervised machine learning algorithms that can perform both regression and classification tasks. The intuition behind the decision tree algorithm is simple, yet also very powerful. . Everyday we need to make numerous decisions, many smalls and a few big. So, Whenever you are in a dilemna, if you&#39;ll keenly observe your thinking process. You&#39;ll find that, you are unconsciously using decision tree approcah or you can also say that decision tree approach is based on our thinking process. . A decision tree split the data into multiple sets.Then each of these sets is further split into subsets to arrive at a decision. | It is a very natural decision making process asking a series of question in a nested if then else statement. | On each node you ask a question to further split the data held by the node. | . So, lets understand what is a decision tree with a help of a real life example. . Consider a scenario where a person asks you to lend them your car for a day, and you have to make a decision whether or not to lend them the car. There are several factors that help determine your decision, some of which have been listed below: . Is this person a close friend or just an acquaintance? . If the person is just an acquaintance, then decline the request; | if the person is friend, then move to next step. | . | Is the person asking for the car for the first time? . If so, lend them the car, | otherwise move to next step. | . | Was the car damaged last time they returned the car? . If yes, decline the request; | if no, lend them the car. | . | The decision tree for the aforementioned scenario looks like this: . The structure of decision tree resembles an upside down tree, with its roots at the top and braches are at the bottom. The end of the branch that doesnt split any more is the decision or leaf. . Now, lets see what is Decision tree algorithm. Decision tree is a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used in classification problems. . It works for both categorical and continuous input and output variables. | In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter / differentiator in input variables. | . . . 4.2 Important Terminology related to Decision Trees . Let’s look at the basic terminology used with Decision trees: . Root Node: It represents entire population or sample and this further gets divided into two or more homogeneous sets. | Splitting: It is a process of dividing a node into two or more sub-nodes. | Decision Node: When a sub-node splits into further sub-nodes, then it is called decision node. | Leaf/ Terminal Node: Nodes do not split is called Leaf or Terminal node. | . Pruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting. | Branch / Sub-Tree: A sub section of entire tree is called branch or sub-tree. | Parent and Child Node: A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node. | . . 4.3 Types of Decision Trees . Types of decision tree is based on the type of target variable we have. It can be of two types: . Categorical Variable Decision Tree: Decision Tree which has categorical target variable then it called as categorical variable decision tree. | . | Continuous Variable Decision Tree: Decision Tree has continuous target variable then it is called as Continuous Variable Decision Tree. | . | . Example: . Let’s say we have a problem to predict whether a customer will pay his renewal premium with an insurance company (Yes/ No). For this we are predicting values for categorical variable. So, the decision tree approach that will be used is Categorical Variable Decision Tree. | Now, suppose insurance company does not have income details for all customers. But, we know that this is an important variable, then we can build a decision tree to predict customer income based on occupation, product and various other variables. In this case, we are predicting values for continuous variable. So , This approach is called Continuous Variable Decision Tree. | . . 4.4 Concept of Homogenity . Homogenous populations are alike and heterogeneous populations are unlike. . A heterogenous population is one where individuals are not similar to one another. | For example, you could have a heterogenous population in terms of humans that have migrated from different regions of the world and currently live together. That population would likely be heterogenous in regards to height, hair texture, disease immunity, and other traits because of the varied background and genetics. | . . Note: In real world you would never get this level of homogeniety. So out of the hetrogenous options you need to select the one having maximum homoginiety. To select the feature which provide maximum homoginety we use gini &amp; entropy techniques. . What Decision tree construction algorithm will try to do is to create a split in such a way that the homogeneity of different pieces must be as high as possible. . Example . Let’s say we have a sample of 30 students with three variables: . Gender (Boy/ Girl) | Class (IX/ X) and, | Height (5 to 6 ft). | 15 out of these 30 play cricket in leisure time. Now, I want to create a model to predict who will play cricket during leisure period? In this problem, we need to segregate students who play cricket in their leisure time based on highly significant input variable among all three. . This is where decision tree helps, it will segregate the students based on all values of three variables and identify the variable, which creates the best homogeneous sets of students (which are heterogeneous to each other). In the snapshot below, you can see that variable Gender is able to identify best homogeneous sets compared to the other two variables. . As mentioned above, decision tree identifies the most significant variable and it’s value that gives best homogeneous sets of population. Now the question which arises is, how does it identify the variable and the split? To do this, decision tree uses various algorithms, which we will shall discuss in the following section. . . 4.5 How does a tree decide where to split? . The decision of making strategic splits heavily affects a tree’s accuracy. The decision criteria is different for classification and regression trees. . Decision trees use multiple algorithms to decide to split a node in two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes. . The algorithm selection is also based on type of target variables. Let’s look at the most commonly used algorithms in decision tree: . . 4.5.1 Gini Index . Gini index says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure. . It works with categorical target variable “Success” or “Failure”. | It performs only Binary splits | Higher the value of Gini higher the homogeneity. | CART (Classification and Regression Tree) uses Gini method to create binary splits. | . Steps to Calculate Gini for a split . Calculate Gini for sub-nodes, using formula sum of square of probability for success and failure (1 - p2 - q2). | Calculate Gini for split using weighted Gini score of each node of that split | Example: – Referring to example used above, where we want to segregate the students based on target variable ( playing cricket or not ). In the snapshot below, we split the population using two input variables Gender and Class. Now, I want to identify which split is producing more homogeneous sub-nodes using Gini index. . Gini for Root node: . 1 - (0.5 * 0.5) - (0.5 * 0.5) = 0.50 | . Split on Gender: . Gini for sub-node Female 1 - (0.2 * 0.2) - (0.8 * 0.8) = 0.32 | . | Gini for sub-node Male 1 - (0.65 * 0.65) - (0.35 * 0.35) = 0.45 | . | Weighted Gini for Split Gender (10/30) * 0.32 + (20/30) * 0.45 = 0.41 | . | Split on Class : . Gini for sub-node Class IX = 1 - (0.43 * 0.43) - (0.57 * 0.57) = 0.49 | . | Gini for sub-node Class X = 1 - (0.56 * 0.56) - (0.44 * 0.44) = 0.49 | . | Calculate weighted Gini for Split Class (14/30) * 0.51 + (16/30) * 0.51 = 0.49 | . | Above, you can see that: Gini score for Split on Gender &lt; Gini score for Split on Class. Also, Gini score for Gender &lt; Gini score for root node. Hence, the node split will take place on Gender. . . 4.5.2 Information Gain: . Look at the image below and think which node can be described easily. I am sure, your answer is C because it requires less information as all values are similar. On the other hand, B requires more information to describe it and A requires the maximum information. In other words, we can say that C is a Pure node, B is less Impure and A is more impure. . Now, we can build a conclusion that: . less impure node requires less information to describe it. | more impure node requires more information. | . Information theory is a measure to define this degree of disorganization in a system by a parameter known as Entropy. . If the sample is completely homogeneous, then the entropy is zero and | If the sample is an equally divided (50% – 50%), it has entropy of one. | . Entropy can be calculated using formula: . where, p &amp; q is probability of success and failure respectively in that node. . Information Gain = 1 - Entropy. | The model will choose the split which facilitates maximum information gain, which in turn means minimum Entropy. | So, it chooses the split which has lowest entropy compared to parent node and other splits. | The lesser the entropy, the better it is. | . Steps to calculate entropy for a split: . Calculate entropy of parent node | Calculate entropy of each individual node of split and | Calculate weighted average of all sub-nodes available in split. | Caluclate the Information Gain in various split options w.r.t parent node | Choose the split with highest Information Gain. | Example: Let’s use this method to identify best split for student example. . Entropy for parent node - (15/30) log2 (15/30) – (15/30) log2 (15/30) = 1. Here 1 shows that it is a impure node. | . | . Entropy for Female node - (2/10) log2 (2/10) – (8/10) log2 (8/10) = 0.72 | . | . Entropy for male node - (13/20) log2 (13/20) – (7/20) log2 (7/20) = 0.93 | . | . Entropy for split Gender = Weighted entropy of sub-nodes (10/30) * 0.72 + (20/30) * 0.93 = 0.86 | . | . . Information Gain for split Gender = Entropy of Parent Node - Weighted entropy for Split Gender 1 - 0.86 = 0.14 | . | . . Entropy for Class IX node, -(6/14) log2 (6/14) – (8/14) log2 (8/14) = 0.99 | . | . Entropy for Class X node, -(9/16) log2 (9/16) – (7/16) log2 (7/16) = 0.99. | . | . Entropy for split Class, (14/30) * 0.99 + (16/30) * 0.99 = 0.99 | . | . . Information Gain for split Class = Entropy of Parent Node - Weighted entropy for Split Class 1 - 0.99 = 0.01 | . | . . Observe that: Information Gain for Split on Gender &gt; Information Gain for Split on Class, So, the tree will split on Gender. . . 4.6 Advantages of using Decision Tree . Easy to Understand: Decision tree output is very easy to understand even for people from non-analytical background. It does not require any statistical knowledge to read and interpret them. | Its graphical representation is very intuitive and users can easily relate their hypothesis. | . | Less data cleaning required: It requires less data cleaning compared to some other modeling techniques. | It is not influenced by outliers and missing values to a fair degree. | . | Data type is not a constraint: It can handle both numerical and categorical variables. | . | Non Parametric Method: Decision tree is considered to be a non-parametric method. This means that decision trees have no assumptions about the space distribution and the classifier structure. | . | . . 4.7 Shortcomings of Decision Trees . Over fitting: Over fitting is one of the most practical difficulty for decision tree models. This problem gets solved by setting constraints on model parameters and pruning (discussed in detailed below). | . | Not a great contributor for regression: While working with continuous numerical variables, decision tree looses information when it categorizes variables in different categories. | . | . . 4.8 Preparing X and y using pandas . X = titanic.loc[:,titanic.columns != &#39;Survived&#39;] X.head() . PassengerId Pclass Age Fare FamilySize GenderClass_female GenderClass_male Embarked_Q Embarked_S . 0 1 | 3 | 22.0 | 7.2500 | 2 | 0 | 1 | 0 | 1 | . 1 2 | 1 | 38.0 | 71.2833 | 2 | 1 | 0 | 0 | 0 | . 2 3 | 3 | 26.0 | 7.9250 | 1 | 1 | 0 | 0 | 1 | . 3 4 | 1 | 35.0 | 53.1000 | 2 | 1 | 0 | 0 | 1 | . 4 5 | 3 | 35.0 | 8.0500 | 1 | 0 | 1 | 0 | 1 | . y = titanic.Survived . . 4.9 Splitting X and y into training and test datasets. . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1) . print(X_train.shape) print(y_train.shape) . (712, 9) (712,) . . 4.10 Decision Tree in scikit-learn . To apply any machine learning algorithm on your dataset, basically there are 4 steps: . Load the algorithm | Instantiate and Fit the model to the training dataset | Prediction on the test set | Calculating the accuracy of the model | The code block given below shows how these steps are carried out: . from sklearn import tree model = tree.DecisionTreeClassifier(criterion=&#39;gini&#39;) model.fit(X, y) predicted= model.predict(x_test) . from sklearn import tree model = tree.DecisionTreeClassifier(random_state = 0) model.fit(X_train, y_train) . DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=0, splitter=&#39;best&#39;) . Plotting our model of decision tree | . import sys !{sys.executable} -m pip install graphviz !{sys.executable} -m pip install pydotplus !{sys.executable} -m pip install Ipython . Requirement already satisfied: graphviz in c: users deepe anaconda3 lib site-packages (0.11) Requirement already satisfied: pydotplus in c: users deepe anaconda3 lib site-packages (2.0.2) Requirement already satisfied: pyparsing&gt;=2.0.1 in c: users deepe anaconda3 lib site-packages (from pydotplus) (2.3.1) Requirement already satisfied: Ipython in c: users deepe anaconda3 lib site-packages (7.4.0) Requirement already satisfied: pickleshare in c: users deepe anaconda3 lib site-packages (from Ipython) (0.7.5) Requirement already satisfied: pygments in c: users deepe anaconda3 lib site-packages (from Ipython) (2.3.1) Requirement already satisfied: prompt-toolkit&lt;2.1.0,&gt;=2.0.0 in c: users deepe anaconda3 lib site-packages (from Ipython) (2.0.9) Requirement already satisfied: decorator in c: users deepe anaconda3 lib site-packages (from Ipython) (4.4.0) Requirement already satisfied: colorama; sys_platform == &#34;win32&#34; in c: users deepe anaconda3 lib site-packages (from Ipython) (0.4.1) Requirement already satisfied: setuptools&gt;=18.5 in c: users deepe anaconda3 lib site-packages (from Ipython) (40.8.0) Requirement already satisfied: traitlets&gt;=4.2 in c: users deepe anaconda3 lib site-packages (from Ipython) (4.3.2) Requirement already satisfied: backcall in c: users deepe anaconda3 lib site-packages (from Ipython) (0.1.0) Requirement already satisfied: jedi&gt;=0.10 in c: users deepe anaconda3 lib site-packages (from Ipython) (0.13.3) Requirement already satisfied: wcwidth in c: users deepe anaconda3 lib site-packages (from prompt-toolkit&lt;2.1.0,&gt;=2.0.0-&gt;Ipython) (0.1.7) Requirement already satisfied: six&gt;=1.9.0 in c: users deepe anaconda3 lib site-packages (from prompt-toolkit&lt;2.1.0,&gt;=2.0.0-&gt;Ipython) (1.12.0) Requirement already satisfied: ipython-genutils in c: users deepe anaconda3 lib site-packages (from traitlets&gt;=4.2-&gt;Ipython) (0.2.0) Requirement already satisfied: parso&gt;=0.3.0 in c: users deepe anaconda3 lib site-packages (from jedi&gt;=0.10-&gt;Ipython) (0.3.4) . import pydotplus from IPython.display import Image dot_tree = tree.export_graphviz(model, out_file=None,filled=True, rounded=True, special_characters=True, feature_names=X.columns) graph = pydotplus.graph_from_dot_data(dot_tree) Image(graph.create_png()) . . 4.11 Using the Model for Prediction . y_pred_train = model.predict(X_train) . y_pred_test = model.predict(X_test) # make predictions on the testing set . Now lets see some model evaluation techniques. | . . 5. Model evaluation . Error is the deviation of the values predicted by the model with the true values. We will use accuracy score and confusion matrix for evaluation. . . 5.1 Model Evaluation using accuracy_score . from sklearn.metrics import accuracy_score print(&#39;Accuracy score for test data is:&#39;, accuracy_score(y_test,y_pred_test)) . Accuracy score for test data is: 0.776536312849162 . . 5.2 Model Evaluation using confusion matrix . A confusion matrix is a summary of prediction results on a classification problem. . The number of correct and incorrect predictions are summarized with count values and broken down by each class. Below is a diagram showing a general confusion matrix. . from sklearn.metrics import confusion_matrix confusion_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred_test)) confusion_matrix.index = [&#39;Actual Died&#39;,&#39;Actual Survived&#39;] confusion_matrix.columns = [&#39;Predicted Died&#39;,&#39;Predicted Survived&#39;] print(confusion_matrix) . Predicted Died Predicted Survived Actual Died 88 18 Actual Survived 22 51 . This means 88 + 51 = 139 correct predictions &amp; 22 + 18 = 40 false predictions. . . 6. Decision Tree with Gridsearch . Applying GridsearchCV method for exhaustive search over specified parameter values of estimator. To know more about the different parameters in decision tree classifier, refer the documentation. Below we will apply gridsearch over the following parameters: . criterion | max_depth | max_features | . You can change other parameters also and compare the impact of it via calculating accuracy score &amp; confusion matrix . from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import GridSearchCV decision_tree_classifier = DecisionTreeClassifier(random_state = 0) tree_para = [{&#39;criterion&#39;:[&#39;gini&#39;,&#39;entropy&#39;],&#39;max_depth&#39;: range(2,60), &#39;max_features&#39;: [&#39;sqrt&#39;, &#39;log2&#39;, None] }] grid_search = GridSearchCV(decision_tree_classifier,tree_para, cv=10, refit=&#39;AUC&#39;) grid_search.fit(X_train, y_train) . C: Users deepe Anaconda3 lib site-packages sklearn model_selection _search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal. DeprecationWarning) . GridSearchCV(cv=10, error_score=&#39;raise-deprecating&#39;, estimator=DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=0, splitter=&#39;best&#39;), fit_params=None, iid=&#39;warn&#39;, n_jobs=None, param_grid=[{&#39;criterion&#39;: [&#39;gini&#39;, &#39;entropy&#39;], &#39;max_depth&#39;: range(2, 60), &#39;max_features&#39;: [&#39;sqrt&#39;, &#39;log2&#39;, None]}], pre_dispatch=&#39;2*n_jobs&#39;, refit=&#39;AUC&#39;, return_train_score=&#39;warn&#39;, scoring=None, verbose=0) . Using the model for prediction | . y_pred_test1 = grid_search.predict(X_test) . Model Evaluation using accuracy_score | . from sklearn.metrics import accuracy_score print(&#39;Accuracy score for test data is:&#39;, accuracy_score(y_test,y_pred_test1)) . Accuracy score for test data is: 0.8044692737430168 . Model Evaluation using confusion matrix | . from sklearn.metrics import confusion_matrix confusion_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred_test1)) confusion_matrix.index = [&#39;Actual Died&#39;,&#39;Actual Survived&#39;] confusion_matrix.columns = [&#39;Predicted Died&#39;,&#39;Predicted Survived&#39;] print(confusion_matrix) . Predicted Died Predicted Survived Actual Died 95 11 Actual Survived 24 49 . You can see 95 + 49 = 144 correct predictions &amp; 24 + 11 = 35 false predictions. . Observations: . With gridsearch accuracy_score increased from 0.765 to 0.804 and the number of correct predictions increased from 139 to 144 and number of false predictions decreased from 40 to 35. | .",
            "url": "https://udaypratapyati.github.io/MachineLearningBlogs/decision%20tree/dt/machinelearning/ml/datascience/2021/08/10/DecisionTreesTitanicDataset.html",
            "relUrl": "/decision%20tree/dt/machinelearning/ml/datascience/2021/08/10/DecisionTreesTitanicDataset.html",
            "date": " • Aug 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "LOGISTIC REGRESSION (Titanic dataset)",
            "content": "1. Problem Statement . The goal is to predict survival of passengers travelling in RMS Titanic using Logistic regression. . . 2. Data Loading and Description . . The dataset consists of the information about people boarding the famous RMS Titanic. Various variables present in the dataset includes data of age, sex, fare, ticket etc. | The dataset comprises of 891 observations of 12 columns. Below is a table showing names of all the columns and their description. | . Column Name Description . PassengerId | Passenger Identity | . Survived | Whether passenger survived or not | . Pclass | Class of ticket | . Name | Name of passenger | . Sex | Sex of passenger | . Age | Age of passenger | . SibSp | Number of sibling and/or spouse travelling with passenger | . Parch | Number of parent and/or children travelling with passenger | . Ticket | Ticket number | . Fare | Price of ticket | . Cabin | Cabin number | . Importing packages . import numpy as np # Implemennts milti-dimensional array and matrices import pandas as pd # For data manipulation and analysis # import pandas_profiling import matplotlib.pyplot as plt # Plotting library for Python programming language and it&#39;s numerical mathematics extension NumPy import seaborn as sns # Provides a high level interface for drawing attractive and informative statistical graphics %matplotlib inline sns.set() from subprocess import check_output . Importing the Dataset . titanic_data = pd.read_csv(&quot;LogReg/titanic_train.csv&quot;) # Importing training dataset using pd.read_csv . . 3. Preprocessing the data . Dealing with missing values Dropping/Replacing missing entries of Embarked. | Replacing missing values of Age and Fare with median values. | Dropping the column &#39;Cabin&#39; as it has too many null values. | . | . titanic_data.Embarked = titanic_data.Embarked.fillna(titanic_data[&#39;Embarked&#39;].mode()[0]) . median_age = titanic_data.Age.median() median_fare = titanic_data.Fare.median() titanic_data.Age.fillna(median_age, inplace = True) titanic_data.Fare.fillna(median_fare, inplace = True) . titanic_data.drop(&#39;Cabin&#39;, axis = 1,inplace = True) . Creating a new feature named FamilySize. | . titanic_data[&#39;FamilySize&#39;] = titanic_data[&#39;SibSp&#39;] + titanic_data[&#39;Parch&#39;]+1 . Segmenting Sex column as per Age, Age less than 15 as Child, Age greater than 15 as Males and Females as per their gender. | . titanic_data[&#39;GenderClass&#39;] = titanic_data.apply(lambda x: &#39;child&#39; if x[&#39;Age&#39;] &lt; 15 else x[&#39;Sex&#39;],axis=1) . titanic_data[titanic_data.Age&lt;15].head(2) . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Embarked FamilySize GenderClass . 7 8 | 0 | 3 | Palsson, Master. Gosta Leonard | male | 2.0 | 3 | 1 | 349909 | 21.0750 | S | 5 | child | . 9 10 | 1 | 2 | Nasser, Mrs. Nicholas (Adele Achem) | female | 14.0 | 1 | 0 | 237736 | 30.0708 | C | 2 | child | . titanic_data[titanic_data.Age&gt;15].head(2) . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Embarked FamilySize GenderClass . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | S | 2 | male | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C | 2 | female | . Dummification of GenderClass &amp; Embarked. | . titanic_data = pd.get_dummies(titanic_data, columns=[&#39;GenderClass&#39;,&#39;Embarked&#39;], drop_first=True) . Dropping columns &#39;Name&#39; , &#39;Ticket&#39; , &#39;Sex&#39; , &#39;SibSp&#39; and &#39;Parch&#39; | . titanic = titanic_data.drop([&#39;Name&#39;,&#39;Ticket&#39;,&#39;Sex&#39;,&#39;SibSp&#39;,&#39;Parch&#39;], axis = 1) titanic.head() . PassengerId Survived Pclass Age Fare FamilySize GenderClass_female GenderClass_male Embarked_Q Embarked_S . 0 1 | 0 | 3 | 22.0 | 7.2500 | 2 | 0 | 1 | 0 | 1 | . 1 2 | 1 | 1 | 38.0 | 71.2833 | 2 | 1 | 0 | 0 | 0 | . 2 3 | 1 | 3 | 26.0 | 7.9250 | 1 | 1 | 0 | 0 | 1 | . 3 4 | 1 | 1 | 35.0 | 53.1000 | 2 | 1 | 0 | 0 | 1 | . 4 5 | 0 | 3 | 35.0 | 8.0500 | 1 | 0 | 1 | 0 | 1 | . Drawing pair plot to know the joint relationship between &#39;Fare&#39; , &#39;Age&#39; , &#39;Pclass&#39; &amp; &#39;Survived&#39; . sns.pairplot(titanic_data[[&quot;Fare&quot;,&quot;Age&quot;,&quot;Pclass&quot;,&quot;Survived&quot;]],vars = [&quot;Fare&quot;,&quot;Age&quot;,&quot;Pclass&quot;],hue=&quot;Survived&quot;, dropna=True,markers=[&quot;o&quot;, &quot;s&quot;]) plt.title(&#39;Pair Plot&#39;) . Text(0.5, 1, &#39;Pair Plot&#39;) . Observing the diagonal elements, . More people of Pclass 1 survived than died (First peak of red is higher than blue) | More people of Pclass 3 died than survived (Third peak of blue is higher than red) | More people of age group 20-40 died than survived. | Most of the people paying less fare died. | . Establishing coorelation between all the features using heatmap. . corr = titanic_data.corr() plt.figure(figsize=(10,10)) sns.heatmap(corr,vmax=.8,linewidth=.01, square = True, annot = True,cmap=&#39;YlGnBu&#39;,linecolor =&#39;black&#39;) plt.title(&#39;Correlation between features&#39;) . Text(0.5, 1, &#39;Correlation between features&#39;) . Age and Pclass are negatively corelated with Survived. | FamilySize is made from Parch and SibSb only therefore high positive corelation among them. | Fare and FamilySize are positively coorelated with Survived. | With high corelation we face redundancy issues. | . . 4. Logistic Regression . . 4.1 Introduction to Logistic Regression . Logistic regression is a techinque used for solving the classification problem. And Classification is nothing but a problem of identifing to which of a set of categories a new observation belongs, on the basis of training dataset containing observations (or instances) whose categorical membership is known. For example to predict: Whether an email is spam (1) or not (0) or, Whether the tumor is malignant (1) or not (0) Below is the pictorial representation of a basic logistic regression model to classify set of images into happy or sad. . Both Linear regression and Logistic regression are supervised learning techinques. But for the Regression problem the output is continuous unlike the classification problem where the output is discrete. . Logistic Regression is used when the dependent variable(target) is categorical. | Sigmoid function or logistic function is used as hypothesis function for logistic regression. Below is a figure showing the difference between linear regression and logistic regression, Also notice that logistic regression produces a logistic curve, which is limited to values between 0 and 1. | . . 4.2 Mathematics behind Logistic Regression . The odds for an event is the (probability of an event occuring) / (probability of event not occuring): For Linear regression: continuous response is modeled as a linear combination of the features: y = β0 + β1x For Logistic regression: log-odds of a categorical response being &quot;true&quot; (1) is modeled as a linear combination of the features: . This is called the logit function. On solving for probability (p) you will get: . . . Shown below is the plot showing linear model and logistic model: . . In other words: . Logistic regression outputs the probabilities of a specific class. | Those probabilities can be converted into class predictions. | . The logistic function has some nice properties: . Takes on an &quot;s&quot; shape | Output is bounded by 0 and 1 | . We have covered how this works for binary classification problems (two response classes). But what about multi-class classification problems (more than two response classes)? . Most common solution for classification models is &quot;one-vs-all&quot; (also known as &quot;one-vs-rest&quot;): decompose the problem into multiple binary classification problems. | Multinomial logistic regression can solve this as a single problem. | . . 4.3 Applications of Logistic Regression . Logistic Regression was used in biological sciences in early twentieth century. It was then used in many social science applications. For instance, . The Trauma and Injury Severity Score (TRISS), which is widely used to predict mortality in injured patients, was originally developed by Boyd et al. using logistic regression. | Many other medical scales used to assess severity of a patient have been developed using logistic regression. | Logistic regression may be used to predict the risk of developing a given disease (e.g. diabetes; coronary heart disease), based on observed characteristics of the patient (age, sex, body mass index, results of various blood tests, etc.). | . Now a days, Logistic Regression have the following applications . Image segementation and categorization | Geographic image processing | Handwriting recognition | Detection of myocardinal infarction | Predict whether a person is depressed or not based on a bag of words from corpus. | The reason why logistic regression is widely used despite of the state of the art of deep neural network is that logistic regression is very efficient and does not require too much computational resources, which makes it affordable to run on production. . . 4.4 Preparing X and y using pandas . X = titanic.loc[:,titanic.columns != &#39;Survived&#39;] X.head() . PassengerId Pclass Age Fare FamilySize GenderClass_female GenderClass_male Embarked_Q Embarked_S . 0 1 | 3 | 22.0 | 7.2500 | 2 | 0 | 1 | 0 | 1 | . 1 2 | 1 | 38.0 | 71.2833 | 2 | 1 | 0 | 0 | 0 | . 2 3 | 3 | 26.0 | 7.9250 | 1 | 1 | 0 | 0 | 1 | . 3 4 | 1 | 35.0 | 53.1000 | 2 | 1 | 0 | 0 | 1 | . 4 5 | 3 | 35.0 | 8.0500 | 1 | 0 | 1 | 0 | 1 | . y = titanic.Survived . . 4.5 Splitting X and y into training and test datasets. . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1) . print(X_train.shape) print(y_train.shape) . (712, 9) (712,) . . 4.6 Logistic regression in scikit-learn . To apply any machine learning algorithm on your dataset, basically there are 4 steps: . Load the algorithm | Instantiate and Fit the model to the training dataset | Prediction on the test set | Calculating the accuracy of the model | The code block given below shows how these steps are carried out: . from sklearn.linear_model import LogisticRegression logreg = LogisticRegression() logreg.fit(X_train, y_train) accuracy_score(y_test,y_pred_test)) . from sklearn.linear_model import LogisticRegression logreg = LogisticRegression() logreg.fit(X_train,y_train) . C: Users prata AppData Roaming Python Python37 site-packages sklearn linear_model least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here. Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations eps=np.finfo(np.float).eps, C: Users prata AppData Roaming Python Python37 site-packages sklearn linear_model least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here. Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations eps=np.finfo(np.float).eps, copy_X=True, fit_path=True, C: Users prata AppData Roaming Python Python37 site-packages sklearn linear_model least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here. Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations eps=np.finfo(np.float).eps, copy_X=True, fit_path=True, C: Users prata AppData Roaming Python Python37 site-packages sklearn linear_model least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here. Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations eps=np.finfo(np.float).eps, positive=False): C: Users prata AppData Roaming Python Python37 site-packages sklearn linear_model least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here. Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps, C: Users prata AppData Roaming Python Python37 site-packages sklearn linear_model least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here. Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps, C: Users prata AppData Roaming Python Python37 site-packages sklearn linear_model least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here. Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations eps=np.finfo(np.float).eps, copy_X=True, positive=False): C: Users prata AppData Roaming Python Python37 site-packages sklearn linear_model randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here. Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations precompute=False, eps=np.finfo(np.float).eps, C: Users prata AppData Roaming Python Python37 site-packages sklearn linear_model randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here. Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations eps=np.finfo(np.float).eps, random_state=None, C: Users prata AppData Roaming Python Python37 site-packages sklearn linear_model randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here. Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations eps=4 * np.finfo(np.float).eps, n_jobs=None, C: Users prata AppData Roaming Python Python37 site-packages sklearn linear_model logistic.py:433: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning. FutureWarning) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=&#39;warn&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;warn&#39;, tol=0.0001, verbose=0, warm_start=False) . . 4.7 Using the Model for Prediction . y_pred_train = logreg.predict(X_train) . C: Users prata AppData Roaming Python Python37 site-packages sklearn linear_model base.py:283: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information. Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations indices = (scores &gt; 0).astype(np.int) . y_pred_test = logreg.predict(X_test) # make predictions on the testing set . C: Users prata AppData Roaming Python Python37 site-packages sklearn linear_model base.py:283: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information. Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations indices = (scores &gt; 0).astype(np.int) . We need an evaluation metric in order to compare our predictions with the actual values. | . . 5. Model evaluation . Error is the deviation of the values predicted by the model with the true values. We will use accuracy score and confusion matrix for evaluation. . . 5.1 Model Evaluation using accuracy classification score . from sklearn.metrics import accuracy_score print(&#39;Accuracy score for test data is:&#39;, accuracy_score(y_test,y_pred_test)) . Accuracy score for test data is: 0.7988826815642458 . . 5.2 Model Evaluation using confusion matrix . A confusion matrix is a summary of prediction results on a classification problem. . The number of correct and incorrect predictions are summarized with count values and broken down by each class. Below is a diagram showing a general confusion matrix. . from sklearn.metrics import confusion_matrix confusion_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred_test)) print(confusion_matrix) . 0 1 0 95 11 1 25 48 . confusion_matrix.index = [&#39;Actual Died&#39;,&#39;Actual Survived&#39;] confusion_matrix.columns = [&#39;Predicted Died&#39;,&#39;Predicted Survived&#39;] print(confusion_matrix) . Predicted Died Predicted Survived Actual Died 95 11 Actual Survived 25 48 . This means 93 + 48 = 141 correct predictions &amp; 25 + 13 = 38 false predictions. . Adjusting Threshold for predicting Died or Survived. . In the section 4.7 we have used, .predict method for classification. This method takes 0.5 as the default threshhod for prediction. | Now, we are going to see the impact of changing threshold on the accuracy of our logistic regression model. | For this we are going to use .predict_proba method instead of using .predict method. | . Setting the threshold to 0.75 . preds1 = np.where(logreg.predict_proba(X_test)[:,1]&gt; 0.75,1,0) print(&#39;Accuracy score for test data is:&#39;, accuracy_score(y_test,preds1)) . Accuracy score for test data is: 0.7374301675977654 . The accuracy have been reduced significantly changing from 0.79 to 0.73. Hence, 0.75 is not a good threshold for our model. . Setting the threshold to 0.25 . preds2 = np.where(logreg.predict_proba(X_test)[:,1]&gt; 0.25,1,0) print(&#39;Accuracy score for test data is:&#39;, accuracy_score(y_test,preds2)) . Accuracy score for test data is: 0.7486033519553073 . The accuracy have been reduced, changing from 0.79 to 0.75. Hence, 0.25 is also not a good threshold for our model. Later on we will see methods to identify the best threshold. .",
            "url": "https://udaypratapyati.github.io/MachineLearningBlogs/logistic%20regression/logreg/machinelearning/ml/datascience/2021/08/09/LogisticRegressionTitanicDataset.html",
            "relUrl": "/logistic%20regression/logreg/machinelearning/ml/datascience/2021/08/09/LogisticRegressionTitanicDataset.html",
            "date": " • Aug 9, 2021"
        }
        
    
  
    
  
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://udaypratapyati.github.io/MachineLearningBlogs/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://udaypratapyati.github.io/MachineLearningBlogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://udaypratapyati.github.io/MachineLearningBlogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}